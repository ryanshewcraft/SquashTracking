{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import sys\n",
    "from imutils.video import VideoStream\n",
    "import argparse\n",
    "import imutils\n",
    "import time\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as matimg\n",
    "import dlib\n",
    "import seaborn as sns\n",
    "% matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# object detection\n",
    "\n",
    "# initialize the list of class labels MobileNet SSD was trained to\n",
    "# detect\n",
    "CLASSES = [\"background\", \"aeroplane\", \"bicycle\", \"bird\", \"boat\",\n",
    "    \"bottle\", \"bus\", \"car\", \"cat\", \"chair\", \"cow\", \"diningtable\",\n",
    "    \"dog\", \"horse\", \"motorbike\", \"person\", \"pottedplant\", \"sheep\",\n",
    "    \"sofa\", \"train\", \"tvmonitor\"]\n",
    " \n",
    "# load our serialized model from disk\n",
    "net = cv2.dnn.readNetFromCaffe(\"mobilenet_ssd\\MobileNetSSD_deploy.prototxt\", \n",
    "                              \"mobilenet_ssd\\MobileNetSSD_deploy.caffemodel\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function does the initial tracking. It alternates bewteen object detection (using a pretrained neural network model from https://github.com/Zehaos/MobileNet) and object tracking. Running the deep neural network for object detection is computationally intensive and thus increases processing time, so it is only used occassionally to identify the players and ensure that both are being tracked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def playerTracker(vidFH, skipFrames):\n",
    "    # INPUTS:\n",
    "    #  vidFH = file handle for video to analyze\n",
    "    #  skipFrames = number of frames to run tracker between object detection\n",
    "    #\n",
    "    # OUTPUTS:\n",
    "    #  playerPositions: [frames x players x [X, Y]]. [X, Y] is the position of the bottom of the\n",
    "    #   detected player box,approximately at their feet.\n",
    "    #  playerBoxes: [frames x players x [X, Y, W, H]]. [X, Y] is the center of the detected player\n",
    "    #    box. [W, H] is the width and height of the box.\n",
    "\n",
    "    nDetected = 0\n",
    "    totalFrames = 0\n",
    "    trackers = []\n",
    "    W = None\n",
    "    H = None\n",
    "\n",
    "    vs = cv2.VideoCapture(vidFH)\n",
    "\n",
    "    playerPositions = np.zeros([1,2,2])\n",
    "    playerBoxes = np.zeros([1,2,4])\n",
    "\n",
    "    # loop over frames from the video stream\n",
    "    while True:\n",
    "        frame = vs.read()\n",
    "        frame = frame[1]\n",
    "\n",
    "        if frame is None:\n",
    "            break\n",
    "\n",
    "        frame = imutils.resize(frame, width=500)\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if W is None or H is None:\n",
    "            (H, W) = frame.shape[:2]\n",
    "\n",
    "        status = \"Waiting\"\n",
    "        rects = []\n",
    "\n",
    "        tmpBox = np.zeros([2,4])\n",
    "        # run detection every 10th frame of if less than 2 players are detected\n",
    "        if totalFrames % skipFrames == 0 or nDetected<2:\n",
    "            status = \"Detecting\"\n",
    "            # create tracker to be used after detection\n",
    "            trackers = cv2.MultiTracker_create()\n",
    "\n",
    "            blob = cv2.dnn.blobFromImage(frame, 0.007843, (W, H), 127.5)\n",
    "            net.setInput(blob)\n",
    "            detections = net.forward()\n",
    "\n",
    "            #determine confidence level that detects two people\n",
    "            tmpConfidence = detections[0,0,:,2]\n",
    "            tmpClasses = detections[0,0,:,1]\n",
    "            persons = np.where(tmpClasses==15)\n",
    "            persons = persons[0]\n",
    "            personConfidence = tmpConfidence[persons]\n",
    "            personConfidence = np.sort(personConfidence)\n",
    "            if len(personConfidence)>=2:\n",
    "                confidenceLevel = personConfidence[-2]-0.01\n",
    "                nDetected = 2\n",
    "            elif len(personConfidence)==1:\n",
    "                confidenceLevel = personConfidence[0]-0.01\n",
    "                nDetected = 1\n",
    "            else:\n",
    "                confidenceLevel = 1\n",
    "                nDetected = 0\n",
    "\n",
    "            for i in np.arange(0, detections.shape[2]):\n",
    "                confidence = detections[0, 0, i, 2]\n",
    "                \n",
    "                if confidence > confidenceLevel:\n",
    "                    idx = int(detections[0, 0, i, 1])\n",
    "\n",
    "                    if CLASSES[idx] != \"person\":\n",
    "                        continue\n",
    "\n",
    "                    box = detections[0, 0, i, 3:7] * np.array([W, H, W, H])\n",
    "                    (startX, startY, endX, endY) = box.astype(\"int\")\n",
    "                    box = (startX, startY, endX-startX, endY-startY)\n",
    "                    rects.append(box)\n",
    "\n",
    "                    # create tracker to be used after detection\n",
    "                    tracker = cv2.TrackerCSRT_create()\n",
    "                    trackers.add(tracker, frame, box)\n",
    "\n",
    "                    (x, y, w, h) = [startX, startY, endX-startX, endY-startY]\n",
    "                    cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "                    if i<2:\n",
    "                        tmpBox[i,:] = [x, y, w, h]\n",
    "\n",
    "        else:\n",
    "            status = \"Tracking\"\n",
    "\n",
    "            # update trackers using new video frame\n",
    "            (success, boxes) = trackers.update(frame)\n",
    "\n",
    "            # loop over the bounding boxes and draw them on the frame\n",
    "            for box in boxes:\n",
    "                (x, y, w, h) = [int(v) for v in box]\n",
    "                cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "                rects.append(box)\n",
    "            nDetected = len(boxes)\n",
    "            tmpBox = rects\n",
    "\n",
    "        info = [\n",
    "            (\"Status: \", status),\n",
    "            (\"Frame\", totalFrames)\n",
    "        ]\n",
    "\n",
    "        tmpPos = np.zeros([2,2])\n",
    "        # compute \"feet\" position and save\n",
    "        for iBox in range(0,np.min([np.shape(rects)[0],2])):\n",
    "            text = \"Player %d\" % int(iBox)\n",
    "            box = rects[iBox]\n",
    "            (x, y, w, h) = [int(v) for v in box]\n",
    "            X = x+int(w/2)\n",
    "            Y = y+int(h/2)\n",
    "            tmpPos[iBox,0] = X\n",
    "            tmpPos[iBox,1] = Y\n",
    "            cv2.putText(frame, text, (X, Y),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 0, 255), 2)\n",
    "\n",
    "        tmpPos = [[tmpBox[0][0]+tmpBox[0][2]/2, tmpBox[0][1]+tmpBox[0][3]],\n",
    "                  [tmpBox[1][0]+tmpBox[1][2]/2, tmpBox[1][1]+tmpBox[1][3]]]\n",
    "        playerPositions[-1,:,:] = tmpPos\n",
    "        playerBoxes[-1,:,:] = tmpBox[0:2]\n",
    "        playerPositions = np.append(playerPositions,np.zeros([1,2,2]),axis=0)\n",
    "        playerBoxes = np.append(playerBoxes,np.zeros([1,2,4]), axis=0)\n",
    "\n",
    "        # loop over the text tuples and draw them on our frame\n",
    "        for (i, (k, v)) in enumerate(info):\n",
    "            text = \"{}: {}\".format(k, v)\n",
    "            cv2.putText(frame, text, (10, H - ((i * 20) + 20)),\n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 0, 255), 2)\n",
    "\n",
    "        # show the output frame\n",
    "        cv2.imshow(\"Frame\", frame)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        # if the `q` key was pressed, break from the loop\n",
    "        if key == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "        totalFrames += 1\n",
    "\n",
    "    vs.release()\n",
    "\n",
    "    # close all windows\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    return playerPositions, playerBoxes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initial tracker doesn't always identify two players. For example, when one player crosses in front of the other, the detection algorithm may only detect a single player or the tracker may drop the occluded player. I'll fix this by perfoming a linear interpolation where the tracker fails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixDropouts(playerPositions):\n",
    "    # INPUTS:\n",
    "    #  playerPositions: output from playerTracker\n",
    "    #\n",
    "    # OUTPUTS:\n",
    "    #  playerPositionsFixed: same as playerPositions with missing data interpolated\n",
    "\n",
    "    playerPositionsFixed = playerPositions.copy()\n",
    "    for i in range(0,2):\n",
    "        playerPosX = playerPositionsFixed[:,i,0]\n",
    "        dropOuts = np.where(np.bitwise_and(playerPosX[0:-1]>1, playerPosX[1:]<=1))\n",
    "        dropOuts = dropOuts[0]\n",
    "        returns = np.where(np.bitwise_and(playerPosX[0:-1]<1, playerPosX[1:]>=1))\n",
    "        returns = returns[0]\n",
    "        for iDrop in range(0,min(len(dropOuts), len(returns))):\n",
    "            startPos = playerPositionsFixed[dropOuts[iDrop],i,:]\n",
    "            endPos = playerPositionsFixed[returns[iDrop]+1,i,:]\n",
    "            interpX = np.linspace(startPos[0], endPos[0], returns[iDrop]-dropOuts[iDrop]+2)\n",
    "            interpY = np.linspace(startPos[1], endPos[1], returns[iDrop]-dropOuts[iDrop]+2)\n",
    "            playerPositionsFixed[dropOuts[iDrop]:returns[iDrop]+2,i,0] = interpX\n",
    "            playerPositionsFixed[dropOuts[iDrop]:returns[iDrop]+2,i,1] = interpY\n",
    "    \n",
    "    return playerPositionsFixed\n",
    "\n",
    "def fixDropouts_Box(playerBoxes):\n",
    "    # INPUTS:\n",
    "    #  playerBoxes: output from playerTracker\n",
    "    #\n",
    "    # OUTPUTS:\n",
    "    #  playerBoxessFixed: same as playerBoxes with missing data interpolated\n",
    "\n",
    "    playerBoxesFixed = playerBoxes.copy()\n",
    "    # fix dropout--replace with linear map\n",
    "    for i in range(0,2):\n",
    "        playerPosX = playerBoxesFixed[:,i,0]\n",
    "        dropOuts = np.where(np.bitwise_and(playerPosX[0:-1]>1, playerPosX[1:]<=1))\n",
    "        dropOuts = dropOuts[0]\n",
    "        returns = np.where(np.bitwise_and(playerPosX[0:-1]<1, playerPosX[1:]>=1))\n",
    "        returns = returns[0]\n",
    "        for iDrop in range(0,min(len(dropOuts), len(returns))):\n",
    "            startPos = playerBoxesFixed[dropOuts[iDrop],i,:]\n",
    "            endPos = playerBoxesFixed[returns[iDrop]+1,i,:]\n",
    "            interpX = np.linspace(startPos[0], endPos[0], returns[iDrop]-dropOuts[iDrop]+2)\n",
    "            interpY = np.linspace(startPos[1], endPos[1], returns[iDrop]-dropOuts[iDrop]+2)\n",
    "            playerBoxesFixed[dropOuts[iDrop]:returns[iDrop]+2,i,0] = interpX\n",
    "            playerBoxesFixed[dropOuts[iDrop]:returns[iDrop]+2,i,1] = interpY\n",
    "            playerBoxesFixed[dropOuts[iDrop]:returns[iDrop]+2,i,2] = playerBoxesFixed[dropOuts[iDrop]-1,i,2]\n",
    "            playerBoxesFixed[dropOuts[iDrop]:returns[iDrop]+2,i,3] = playerBoxesFixed[dropOuts[iDrop]-1,i,3]\n",
    "               \n",
    "    return playerBoxesFixed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition to dropping players, the tracker often switches the labels of the two players. The next function sets the positions and boxes so that playersPositions[:,i,:] corresponds consistently to player i. It does this by setting each players identity based on the histograms for the R, G, and B channels in the initial detections boxes. This works best when the players are wearing different color shirts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def identifyPlayer(playerPositions, playerBoxes, vidFH):\n",
    "    # INPUTS:\n",
    "    #  playerPositions: output from playerTracker or fixDropouts\n",
    "    #  playerBoxes: output from playerTracker or fixDropouts_Box\n",
    "    #  vidFH: file handle for video that was analyzed\n",
    "    #\n",
    "    # OUTPUTS:\n",
    "    #  playerPositionsFixed: identity-corrected positions\n",
    "    \n",
    "    playerPositionsFixed = playerPositions.copy()\n",
    "    pB = playerBoxes.copy()\n",
    "\n",
    "    #get players initial range\n",
    "    vs = cv2.VideoCapture(vidFH)\n",
    "    frame = vs.read()\n",
    "    frame = frame[1]\n",
    "    frame = imutils.resize(frame, width=500)\n",
    "    rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    player0 = frame[int(pB[0,0,1]):int(pB[0,0,1]+pB[0,0,3]),\n",
    "                    int(pB[0,0,0]):int(pB[0,0,0]+pB[0,0,2])]\n",
    "    player1 = frame[int(pB[0,1,1]):int(pB[0,1,1]+pB[0,1,3]),\n",
    "                    int(pB[0,1,0]):int(pB[0,1,0]+pB[0,1,2])]\n",
    "\n",
    "    h0R,b = np.histogram(player0[:,:,0].ravel(),256,[0,256])\n",
    "    h0G,b = np.histogram(player0[:,:,1].ravel(),256,[0,256])\n",
    "    h0B,b = np.histogram(player0[:,:,2].ravel(),256,[0,256])\n",
    "    h1R,b = np.histogram(player1[:,:,0].ravel(),256,[0,256])\n",
    "    h1G,b = np.histogram(player1[:,:,1].ravel(),256,[0,256])\n",
    "    h1B,b = np.histogram(player1[:,:,2].ravel(),256,[0,256])\n",
    "\n",
    "    totalFrames = 1\n",
    "\n",
    "    # loop over frames and test detected player aganst initial histograms\n",
    "    while True:\n",
    "        frame = vs.read()\n",
    "        frame = frame[1]\n",
    "\n",
    "        if frame is None:\n",
    "            break\n",
    "\n",
    "        frame = imutils.resize(frame, width=500)\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        try:\n",
    "            player0 = frame[int(pB[totalFrames,0,1]):int(pB[totalFrames,0,1]+pB[totalFrames,0,3]),\n",
    "                            int(pB[totalFrames,0,0]):int(pB[totalFrames,0,0]+pB[totalFrames,0,2])]\n",
    "            player1 = frame[int(pB[totalFrames,1,1]):int(pB[totalFrames,1,1]+pB[totalFrames,1,3]),\n",
    "                            int(pB[totalFrames,1,0]):int(pB[totalFrames,1,0]+pB[totalFrames,1,2])]\n",
    "\n",
    "            h0R_tmp,b = np.histogram(player0[:,:,0].ravel(),256,[0,256])\n",
    "            h0G_tmp,b = np.histogram(player0[:,:,1].ravel(),256,[0,256])\n",
    "            h0B_tmp,b = np.histogram(player0[:,:,2].ravel(),256,[0,256])\n",
    "            h1R_tmp,b = np.histogram(player1[:,:,0].ravel(),256,[0,256])\n",
    "            h1G_tmp,b = np.histogram(player1[:,:,1].ravel(),256,[0,256])\n",
    "            h1B_tmp,b = np.histogram(player1[:,:,2].ravel(),256,[0,256])\n",
    "\n",
    "            dist0R = np.linalg.norm(h0R_tmp-h0R)\n",
    "            dist0G = np.linalg.norm(h0G_tmp-h0G)\n",
    "            dist0B = np.linalg.norm(h0B_tmp-h0B)\n",
    "            dist1R = np.linalg.norm(h0R_tmp-h1R)\n",
    "            dist1G = np.linalg.norm(h0G_tmp-h1G)\n",
    "            dist1B = np.linalg.norm(h0B_tmp-h1B)\n",
    "\n",
    "            dist0 = np.mean([dist0R,dist0G,dist0B])\n",
    "            dist1 = np.mean([dist1R,dist1G,dist1B])\n",
    "\n",
    "            if dist0<dist1:\n",
    "                playerIDs = [0, 1]\n",
    "            else:\n",
    "                playerIDs = [1, 0]\n",
    "\n",
    "            playerPositionsFixed[totalFrames,0,:] = playerPositions[totalFrames,int(playerIDs[0]),:]\n",
    "            playerPositionsFixed[totalFrames,1,:] = playerPositions[totalFrames,int(playerIDs[1]),:]\n",
    "        except:\n",
    "            badWindow = 1\n",
    "\n",
    "        totalFrames += 1\n",
    "\n",
    "    vs.release()\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    \n",
    "    return playerPositionsFixed\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next function plots a heatmap of the player positions on an image from the video. This can be used to show the locations on the court that the players tend to visit.\n",
    "\n",
    "The function can plot the data for a single player at a time using playerPositions[:,i,:] or both players together using simply playerPositions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatMapPositions(vidFH, playerPositions, frameNumber=200):\n",
    "    # INPUTS:\n",
    "    #  vidFH: file handle for video that was analyzed\n",
    "    #  playerPositions: output from playerTracker or fixDropouts. Can be [time x [X , Y]] or\n",
    "    #   [time x players x [X, Y]]\n",
    "    #  frameNumber: frame for image that is used as the background image. Defaults to 200\n",
    "    #\n",
    "    # OUTPUTS:\n",
    "    #  None\n",
    "    \n",
    "\n",
    "    nSamples = np.shape(playerPositions)[0]\n",
    "    \n",
    "    # get background image to overlay heatmap on\n",
    "    vs = cv2.VideoCapture(vidFH)\n",
    "    for i in range(0, frameNumber):\n",
    "        frame = vs.read()\n",
    "        frame = frame[1]\n",
    "        frame = imutils.resize(frame, width=500)\n",
    "    vs.release()\n",
    "\n",
    "    sImag = np.shape(frame)\n",
    "\n",
    "    # figure out if we're looking at data for one or two players\n",
    "    if len(np.shape(playerPositions))==2:\n",
    "        nPlayers = 1\n",
    "    else:\n",
    "        nPlayers = 2\n",
    "        \n",
    "    # combine data if two players\n",
    "    if nPlayers == 1:\n",
    "        tmpX = playerPositions[0:nSamples,0]\n",
    "        tmpY = playerPositions[0:nSamples,1]\n",
    "    else:\n",
    "        tmpX = np.concatenate((playerPositions[0:nSamples,0,0], playerPositions[0:nSamples,1,0]), axis=0)\n",
    "        tmpY = np.concatenate((playerPositions[0:nSamples,0,1], playerPositions[0:nSamples,1,1]), axis=0)\n",
    "\n",
    "    #compute 2d histogram from location data\n",
    "    playerLocs, xedges, yedges = np.histogram2d(tmpX,tmpY,bins=[int(sImag[0]/10), int(sImag[1]/10)])\n",
    "\n",
    "    # plot over background image\n",
    "    playerLocs = playerLocs.T\n",
    "    fig = plt.figure(figsize=(7, 3))\n",
    "    ax = fig.add_subplot(111)\n",
    "    ax.imshow(playerLocs, interpolation='gaussian', origin='low',\n",
    "               extent=[xedges[0], xedges[-1], yedges[0], yedges[-1]],\n",
    "              alpha=1)\n",
    "    ax.imshow(frame, alpha=0.5)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This final function makes a video with the tracker location plotted for each player."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeVideo(vidFH, playerPositions, vidOut):\n",
    "    # INPUTS:\n",
    "    #  vidFH: file handle for video that was analyzed\n",
    "    #  playerPositions: output from playerTracker/fixDropouts/identifyPlayer\n",
    "    #  vidOut: file name for output video. \".avi\" is automatically added so should not be\n",
    "    #   included in vidOut\n",
    "    #\n",
    "    # OUTPUTS:\n",
    "    #  None\n",
    "    \n",
    "    # initialize output info\n",
    "    vs = cv2.VideoCapture(vidFH)\n",
    "    frame = vs.read()\n",
    "    frame = frame[1]\n",
    "    vs.release()\n",
    "    dims = np.shape(frame)\n",
    "    frame = imutils.resize(frame, width=500)\n",
    "    dims = np.shape(frame)\n",
    "    frame_width = dims[1]\n",
    "    frame_height = dims[0]\n",
    "\n",
    "    vs = cv2.VideoCapture(vidFH)\n",
    "    fourcc = cv2.VideoWriter_fourcc('X','V','I','D')\n",
    "    fhOut = '%s.avi' % vidOut\n",
    "    out = cv2.VideoWriter(fhOut,fourcc, 30.0, (frame_width,frame_height))\n",
    "    totalFrames = 0\n",
    "\n",
    "    W = None\n",
    "    H = None\n",
    "\n",
    "    # draw player positions over frames\n",
    "    while True:\n",
    "        frame = vs.read()\n",
    "        frame = frame[1]\n",
    "\n",
    "        if frame is None:\n",
    "            break\n",
    "\n",
    "        frame = imutils.resize(frame, width=500)\n",
    "        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "        if W is None or H is None:\n",
    "            (H, W) = frame.shape[:2]\n",
    "\n",
    "        for iPlayer in range(0,2):\n",
    "            X = int(playerPositions[totalFrames,iPlayer,0])\n",
    "            Y = int(playerPositions[totalFrames,iPlayer,1])\n",
    "            #text = \"Player %d\" % int(iPlayer)\n",
    "            #cv2.putText(frame, text, (X, Y),\n",
    "            #    cv2.FONT_HERSHEY_SIMPLEX, 0.3, (0, 0, 255), 2)\n",
    "            cv2.rectangle(frame, (X-1, Y-1), (X+1, Y+1), (0, 255, 0), 2)\n",
    "\n",
    "        out.write(frame)\n",
    "\n",
    "        # show the output frame\n",
    "        cv2.imshow(\"Frame\", frame)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        # if the `q` key was pressed, break from the loop\n",
    "        if key == ord(\"q\"):\n",
    "            break\n",
    "\n",
    "        totalFrames += 1\n",
    "\n",
    "    vs.release()\n",
    "    out.release()\n",
    "\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vidFH = \"ProSquash_Cropped.mp4\"\n",
    "vidOut = \"ProSquash_Tracked\"\n",
    "skipFrames = 10\n",
    "\n",
    "playerPositions, playerBoxes = playerTracker(vidFH, skipFrames)\n",
    "playerBoxesFixed = fixDropouts_Box(playerBoxes)\n",
    "playerPositionsFixed = fixDropouts(playerPositions)\n",
    "playerPositionsFixed = identifyPlayer(playerPositionsFixed, playerBoxesFixed, vidFH)\n",
    "heatMapPositions(vidFH, playerPositionsFixed[:,0,:], frameNumber=200)\n",
    "heatMapPositions(vidFH, playerPositionsFixed[:,1,:], frameNumber=200)\n",
    "makeVideo(vidFH, playerPositionsFixed, vidOut)\n",
    "\n",
    "print(\"DONE\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
